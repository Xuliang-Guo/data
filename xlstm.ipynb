{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xuliang-Guo/data/blob/main/xlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CasualConv1D"
      ],
      "metadata": {
        "id": "L3yhECq2eFRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLiA-CpmeDbD"
      },
      "outputs": [],
      "source": [
        "class CausalConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, dilation=1):\n",
        "        super(CausalConv1D, self).__init__()\n",
        "        self.conv = layers.Conv1D(filters, kernel_size,\n",
        "                                  padding='causal', dilation_rate=dilation)\n",
        "    def call(self, x):\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9ffPPacU6mi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BlockDiagonal"
      ],
      "metadata": {
        "id": "e0YJKhNreJei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BlockDiagonal(tf.keras.layers.Layer):\n",
        "    def __init__(self, in_features, out_features, num_blocks):\n",
        "        super(BlockDiagonal, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        assert in_features % num_blocks == 0\n",
        "        assert out_features % num_blocks == 0\n",
        "\n",
        "        block_in_features = in_features // num_blocks\n",
        "        block_out_features = out_features // num_blocks\n",
        "\n",
        "        self.blocks = [\n",
        "            tf.keras.layers.Dense(block_out_features, activation='linear')\n",
        "            for _ in range(num_blocks)\n",
        "        ]\n",
        "\n",
        "    def call(self, x):\n",
        "        x_chunks = tf.split(x, num_or_size_splits=self.num_blocks, axis=-1)\n",
        "        x_chunks = [block(chunk) for block, chunk in zip(self.blocks, x_chunks)]\n",
        "        x = tf.concat(x_chunks, axis=-1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6KKWdehjeLel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sLSTMBlock"
      ],
      "metadata": {
        "id": "2MQp_-XJeN4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sLSTMBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, proj_factor=4/3):\n",
        "        super(sLSTMBlock, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = hidden_size // num_heads\n",
        "        self.proj_factor = proj_factor\n",
        "\n",
        "        assert hidden_size % num_heads == 0\n",
        "        assert proj_factor > 0\n",
        "\n",
        "        self.layer_norm = layers.LayerNormalization()\n",
        "        self.causal_conv = CausalConv1D(1, 4)\n",
        "\n",
        "        self.Wz = BlockDiagonal(input_size, hidden_size, num_heads)\n",
        "        self.Wi = BlockDiagonal(input_size, hidden_size, num_heads)\n",
        "        self.Wf = BlockDiagonal(input_size, hidden_size, num_heads)\n",
        "        self.Wo = BlockDiagonal(input_size, hidden_size, num_heads)\n",
        "\n",
        "        self.Rz = BlockDiagonal(hidden_size, hidden_size, num_heads)\n",
        "        self.Ri = BlockDiagonal(hidden_size, hidden_size, num_heads)\n",
        "        self.Rf = BlockDiagonal(hidden_size, hidden_size, num_heads)\n",
        "        self.Ro = BlockDiagonal(hidden_size, hidden_size, num_heads)\n",
        "\n",
        "        self.group_norm = layers.GroupNormalization(groups=num_heads)\n",
        "\n",
        "        self.up_proj_left = layers.Dense(int(hidden_size * proj_factor), activation='linear')\n",
        "        self.up_proj_right = layers.Dense(int(hidden_size * proj_factor), activation='linear')\n",
        "        self.down_proj = layers.Dense(input_size, activation='linear')\n",
        "\n",
        "    def call(self, x, prev_state):\n",
        "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
        "        x_norm = self.layer_norm(x)\n",
        "        x_conv = tf.nn.silu(tf.squeeze(self.causal_conv(tf.expand_dims(x_norm, axis=-1)), axis=-1))\n",
        "\n",
        "        z = tf.tanh(self.Wz(x) + self.Rz(h_prev))\n",
        "        o = tf.sigmoid(self.Wo(x) + self.Ro(h_prev))\n",
        "        i_tilde = self.Wi(x_conv) + self.Ri(h_prev)\n",
        "        f_tilde = self.Wf(x_conv) + self.Rf(h_prev)\n",
        "\n",
        "        m_t = tf.maximum(f_tilde + m_prev, i_tilde)\n",
        "        i = tf.exp(i_tilde - m_t)\n",
        "        f = tf.exp(f_tilde + m_prev - m_t)\n",
        "\n",
        "        c_t = f * c_prev + i * z\n",
        "        n_t = f * n_prev + i\n",
        "        h_t = o * c_t / n_t\n",
        "\n",
        "        output = h_t\n",
        "        output_norm = self.group_norm(output)\n",
        "        output_left = self.up_proj_left(output_norm)\n",
        "        output_right = self.up_proj_right(output_norm)\n",
        "        output_gated = tf.nn.gelu(output_right)\n",
        "        output = output_left * output_gated\n",
        "        output = self.down_proj(output)\n",
        "        final_output = output + x\n",
        "\n",
        "        return final_output, (h_t, c_t, n_t, m_t)\n"
      ],
      "metadata": {
        "id": "gIJsPFNeeQdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### mLSTMBlock"
      ],
      "metadata": {
        "id": "F852FjE9eQ9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTMBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, proj_factor=2):\n",
        "        super(mLSTMBlock, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = hidden_size // num_heads\n",
        "        self.proj_factor = proj_factor\n",
        "\n",
        "        assert hidden_size % num_heads == 0\n",
        "        assert proj_factor > 0\n",
        "\n",
        "        self.layer_norm = layers.LayerNormalization()\n",
        "        self.up_proj_left = layers.Dense(int(input_size * proj_factor), activation='linear')\n",
        "        self.up_proj_right = layers.Dense(hidden_size, activation='linear')\n",
        "        self.down_proj = layers.Dense(input_size, activation='linear')\n",
        "\n",
        "        self.causal_conv = CausalConv1D(1, 4)\n",
        "        self.skip_connection = layers.Dense(hidden_size, activation='linear')\n",
        "\n",
        "        self.Wq = BlockDiagonal(int(input_size * proj_factor), hidden_size, num_heads)\n",
        "        self.Wk = BlockDiagonal(int(input_size * proj_factor), hidden_size, num_heads)\n",
        "        self.Wv = BlockDiagonal(int(input_size * proj_factor), hidden_size, num_heads)\n",
        "        self.Wi = layers.Dense(hidden_size, activation='linear')\n",
        "        self.Wf = layers.Dense(hidden_size, activation='linear')\n",
        "        self.Wo = layers.Dense(hidden_size, activation='linear')\n",
        "\n",
        "        self.group_norm = layers.GroupNormalization(groups=num_heads)\n",
        "\n",
        "\n",
        "    def call(self, x, prev_state):\n",
        "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
        "        x_norm = self.layer_norm(x)\n",
        "        x_up_left = self.up_proj_left(x_norm)\n",
        "        x_up_right = self.up_proj_right(x_norm)\n",
        "\n",
        "        x_conv = tf.nn.silu(tf.squeeze(self.causal_conv(tf.expand_dims(x_up_left, axis=-1)), axis=-1))\n",
        "\n",
        "        x_skip = self.skip_connection(x_conv)\n",
        "\n",
        "        q = self.Wq(x_conv)\n",
        "        k = self.Wk(x_conv) / (self.head_size ** 0.5)\n",
        "        v = self.Wv(x_up_left)\n",
        "\n",
        "        i_tilde = self.Wi(x_conv)\n",
        "        f_tilde = self.Wf(x_conv)\n",
        "        o = tf.sigmoid(self.Wo(x_up_left))\n",
        "\n",
        "        m_t = tf.math.maximum(f_tilde + m_prev, i_tilde)\n",
        "        i = tf.exp(i_tilde - m_t)\n",
        "        f = tf.exp(f_tilde + m_prev - m_t)\n",
        "\n",
        "        c_t = f * c_prev + i * (v * k) # v @ k.T\n",
        "        n_t = f * n_prev + i * k\n",
        "        h_t = o * (c_t * q) / tf.reduce_max(tf.abs(tf.matmul(tf.transpose(n_t), q)), axis=1) # o * (c @ q) / max{|n.T @ q|, 1}\n",
        "\n",
        "        output = h_t\n",
        "        output_norm = self.group_norm(output)\n",
        "        output = output_norm + x_skip\n",
        "        output = output * tf.nn.silu(x_up_right)\n",
        "        output = self.down_proj(output)\n",
        "        final_output = output + x\n",
        "\n",
        "        return final_output, (h_t, c_t, n_t, m_t)"
      ],
      "metadata": {
        "id": "NfrinO5XeTUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLSTM"
      ],
      "metadata": {
        "id": "aJ30NjrCeT5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. sLSTM"
      ],
      "metadata": {
        "id": "yQojDeoYeVWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class sLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, num_layers=1,  proj_factor=4/3):\n",
        "        super(sLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.proj_factor_slstm = proj_factor\n",
        "\n",
        "        # Create a list of sLSTMBlock layers\n",
        "        self.layers = [sLSTMBlock(input_size, hidden_size, num_heads, proj_factor) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Ensure x is of shape (batch, seq_len, input)\n",
        "        seq_len, batch_size, _ = tf.shape(x)[1], tf.shape(x)[0], tf.shape(x)[2]\n",
        "\n",
        "        h = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        c = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        n = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        m = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "\n",
        "        ta = tf.TensorArray(dtype=tf.float32, size=seq_len)\n",
        "\n",
        "        for t in tf.range(seq_len):\n",
        "            xt = x[:, t, :]\n",
        "            for i in range(self.num_layers):\n",
        "                xt, [h[i], c[i], n[i], m[i]] = self.layers[i](xt, [h[i], c[i], n[i], m[i]])\n",
        "            ta = ta.write(t, xt)\n",
        "\n",
        "        hidden_states = ta.stack()\n",
        "        hidden_states = tf.transpose(hidden_states, [1, 0, 2])\n",
        "\n",
        "        out = hidden_states[:, -1, :]\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "vfFFUyYTeY8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. mLSTM"
      ],
      "metadata": {
        "id": "OQbdoayUeZdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTM(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_size, hidden_size, num_heads, num_layers=1,  proj_factor=2):\n",
        "        super(mLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.proj_factor_slstm = proj_factor\n",
        "\n",
        "        # Create a list of sLSTMBlock layers\n",
        "        self.layers = [mLSTMBlock(input_size, hidden_size, num_heads, proj_factor) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Ensure x is of shape (batch, seq_len, input)\n",
        "        seq_len, batch_size, _ = tf.shape(x)[1], tf.shape(x)[0], tf.shape(x)[2]\n",
        "\n",
        "        h = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        c = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        n = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "        m = [tf.zeros((batch_size, self.hidden_size)) for _ in range(self.num_layers)]\n",
        "\n",
        "        ta = tf.TensorArray(dtype=tf.float32, size=seq_len)\n",
        "\n",
        "        for t in tf.range(seq_len):\n",
        "            xt = x[:, t, :]\n",
        "            for i in range(self.num_layers):\n",
        "                xt, [h[i], c[i], n[i], m[i]] = self.layers[i](xt, [h[i], c[i], n[i], m[i]])\n",
        "            ta = ta.write(t, xt)\n",
        "\n",
        "        hidden_states = ta.stack()\n",
        "        hidden_states = tf.transpose(hidden_states, [1, 0, 2])\n",
        "\n",
        "        out = hidden_states[:, -1, :]\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "CLGX_gszea0A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}